{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np \n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from skimage import transform\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameEnv:\n",
    "    \n",
    "    \"\"\"\n",
    "      This Class creates Atari Game Enivroment and provides some preprocessing functions.\n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(self, game = 'SpaceInvaders-v0'):\n",
    "        self.env = gym.make(game)\n",
    "        self.n_actions = self.env.action_space.n\n",
    "        self.frame_size = self.env.observation_space.shape\n",
    "        self.hot_enc_actions = np.array(np.identity(self.n_actions).tolist()) \n",
    "        self.stack_size = 4\n",
    "        self.stacked_frames = deque([np.zeros((110,84), dtype=np.int) for i in range(self.stack_size)], maxlen=self.stack_size)\n",
    "        self.hyperparameters = {\n",
    "                               'learning_rate' : 0.00025,\n",
    "                               'total_episodes' : 50,\n",
    "                               'max_steps' : 50000,\n",
    "                               'btach_size': 64,\n",
    "                               'explore_start' : 1,\n",
    "                               'explore_end' : 0.01,\n",
    "                               'decay_rate' : 0.00001,\n",
    "                               'gamma' : 0.9,\n",
    "                               'pretrain_length' : 64,\n",
    "                               'memory_size' : 1000000,\n",
    "                               'state_size' : [110, 84, 4]\n",
    "                               }\n",
    "        self.training = False\n",
    "        self.render = False\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def _preprocess_frame(self,frame):\n",
    "        gray_frame = rgb2gray(frame)\n",
    "        cropped_frame = gray_frame[8:-12,4:-12]\n",
    "\n",
    "        # Normalize Pixel Values\n",
    "        normalized_frame = cropped_frame/255.0\n",
    "\n",
    "        # Resize\n",
    "        # Thanks to MikoÅ‚aj Walkowiak\n",
    "        preprocessed_frame = transform.resize(normalized_frame, [110,84])\n",
    "\n",
    "        return preprocessed_frame # 110x84x1 frame\n",
    "  \n",
    "    def stack_frame(self, state, new_epis = False):\n",
    "    \n",
    "        frame = self._preprocess_frame(state)\n",
    "\n",
    "        if new_epis:\n",
    "            self.stacked_frames  =  deque([frame for _ in range(self.stack_size)], maxlen=self.stack_size)\n",
    "        else:\n",
    "            self.stacked_frames.append(frame)\n",
    "\n",
    "        self.stacked_state = np.stack(self.stacked_frames, axis=2)\n",
    "        return self.stacked_state  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNN:\n",
    "  \n",
    "  def __init__(self, gamenv, name):\n",
    "    self.gamenv = gamenv\n",
    "    self.decay_step = 0\n",
    "    with tf.variable_scope(name):\n",
    "        self._inputs = tf.placeholder(tf.float32, [None, *self.gamenv.hyperparameters['state_size']], name='inputs')\n",
    "        self._ISWeights = tf.placeholder(tf.float32, [None,1], name='IS_weights')\n",
    "        self._actions = tf.placeholder(tf.float32, [None, self.gamenv.n_actions], name='actions')\n",
    "        self.target_Q = tf.placeholder(tf.float32, [None], name=\"target\")\n",
    "      \n",
    "        self.conv1 = tf.layers.conv2d(inputs = self._inputs, \n",
    "                                    filters = 32,\n",
    "                                    kernel_size = [8,8],\n",
    "                                    strides = [4,4],\n",
    "                                    padding = 'VALID',\n",
    "                                    kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                    name = 'Conv1')\n",
    "        self.actvf1 = tf.nn.elu(self.conv1, name='Elu1')\n",
    "      \n",
    "        self.conv2 = tf.layers.conv2d(inputs = self.conv1, \n",
    "                                    filters = 64,\n",
    "                                    kernel_size = [4,4],\n",
    "                                    strides = [2,2],\n",
    "                                    padding = 'VALID',\n",
    "                                    kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                    name = 'Conv2')\n",
    "        self.actvf2 = tf.nn.elu(self.conv2, name='Elu2')\n",
    "      \n",
    "        self.conv3 = tf.layers.conv2d(inputs = self.conv2, \n",
    "                                    filters = 64,\n",
    "                                    kernel_size = [3,3],\n",
    "                                    strides = [2,2],\n",
    "                                    padding = 'VALID',\n",
    "                                    kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                    name = 'Conv3')\n",
    "        self.actvf3 = tf.nn.elu(self.conv3, name='Elu3')\n",
    "      \n",
    "        self.flatten = tf.contrib.layers.flatten(self.actvf3)\n",
    "        self.value_fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                        units = 512,\n",
    "                                        activation = tf.nn.elu,\n",
    "                                        kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                                        name = 'Value_fc')\n",
    "        self.value = tf.layers.dense(inputs = self.value_fc,\n",
    "                                    units = 1,\n",
    "                                    activation = None,\n",
    "                                    kernel_initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.advantage_fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                  units = 512,\n",
    "                                  activation = tf.nn.elu,\n",
    "                                       kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                name=\"advantage_fc\")\n",
    "            \n",
    "        self.advantage = tf.layers.dense(inputs = self.advantage_fc,\n",
    "                                        units = self.action_size,\n",
    "                                        activation = None,\n",
    "                                        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                name=\"advantages\")\n",
    "        self.fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                units = 512,\n",
    "                                activation = tf.nn.elu,\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                name=\"fc1\")\n",
    "            \n",
    "        self.output = self.value + tf.subtract(self.advantage, tf.reduce_mean(self.advantage, axis=1, keepdims=True))\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.output, self._actions), axis=1)\n",
    "            \n",
    "            # The loss is the difference between our predicted Q_values and the Q_target\n",
    "            # Sum(Qtarget - Q)^2\n",
    "        self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
    "            \n",
    "        self.optimizer = tf.train.AdamOptimizer(self.gamenv.hyperparameters['learning_rate']).minimize(self.loss)\n",
    "      \n",
    "      \n",
    "    def predict_action(self, state, sess):\n",
    "        hyperp = self.gamenv.hyperparameters\n",
    "        explore_probability = hyperp['explore_end'] + (hyperp['explore_start'] - hyperp['explore_end']) * np.exp(-hyperp['decay_rate'] * self.decay_step)\n",
    "\n",
    "        if explore_probability > np.random.rand():\n",
    "            action = self.gamenv.hot_enc_actions[self.gamenv.env.action_space.sample()]\n",
    "\n",
    "        else:\n",
    "            Qs = sess.run(self.output,feed_dict = {self._inputs:state.reshape((1,*state.shape))})\n",
    "            action = self.gamenv.hot_enc_actions[np.argmax(Qs)]\n",
    "\n",
    "        return action, explore_probability\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
